{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f150f3-4be5-4565-b08f-ef0204c16c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get existing bias models\n",
    "mkdir bias_model\n",
    "wget https://storage.googleapis.com/chrombpnet_data/input_files/bias_models/ATAC/ENCSR868FGK_bias_fold_0.h5 -O bias_model/ENCSR868FGK_bias_fold_0.h5\n",
    "wget https://storage.googleapis.com/chrombpnet_data/input_files/bias_models/ATAC/scATAC_dermal_fibroblast.h5 -O bias_model/scATAC_dermal_fibroblast.h5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92077f-c7bb-45b8-b316-fc8ee2f3539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8412d7-c444-4bb0-8e81-a7d018670a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "mamba activate chrombpnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002f7485-4ad8-4ebc-8268-b6f0b110f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "export PATH=/cluster/home/jjanssens/.local/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1722c3-8b58-4624-8597-50e606ce1e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/home/jjanssens/.local/bin:/cluster/project/treutlein/jjans/software/miniforge3/envs/chrombpnet/bin:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/bin:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/bin:/cluster/apps/gcc-8.2.0/cuda-12.1.1-mpwcqkwqghc7y2at5a6wuuhbgmm6efux/bin:/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva/bin:/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/bin:/cluster/apps/gcc-8.2.0/eccodes-2.21.0-o4xitaateyj4fuopb6chuxme7d5bp4zp/bin:/cluster/apps/gcc-8.2.0/hdf5-1.10.1-qj3ju3qfhvucsk5eevrtb2lehbux5nmv/bin:/cluster/apps/nss/jupyterhub/3.5.1/bin:/cluster/apps/gcc-8.2.0/git-2.31.1-q45wg6avfyvko4weuhmnpghaag45ynoo/bin:/cluster/apps/gcc-8.2.0/npm-6.14.9-774crfohwvu6a33ijcow7x5cvonu44oi/bin:/cluster/apps/gcc-8.2.0/r-4.2.2-ydfaklhfrhw5dy6qcfzxlxfviwovcord/bin:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/bin:/cluster/apps/local:/cluster/apps/sfos/bin:/usr/local/bin:/usr/local/sbin:/usr/sbin:/usr/bin:/sbin:/bin:/cluster/slurm/apps/bin:/cluster/apps/local:/cluster/apps/local\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4942e66-b7e1-4607-a600-03d84c7a8915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib:/cluster/apps/gcc-8.2.0/cudnn-8.9.2.26-ogi7ed2h6ejs7vumekv46idqqas4axgq/lib:/cluster/apps/gcc-8.2.0/cuda-12.1.1-mpwcqkwqghc7y2at5a6wuuhbgmm6efux/lib64:/cluster/apps/gcc-8.2.0/nccl-2.11.4-1-pwkiz23vbeac3vt5ykybdwzaykprizb2/lib:/cluster/apps/gcc-8.2.0/openblas-0.3.15-huwxbhezdzoo74awrgoz6sd2qndpmdva/lib:/cluster/apps/nss/gcc-8.2.0/python/3.10.4/x86_64/lib64:/cluster/apps/gcc-8.2.0/zlib-1.2.9-roj3c3p7lbd2kn3gstlt4rxdcgvb3csi/lib:/cluster/apps/gcc-8.2.0/eccodes-2.21.0-o4xitaateyj4fuopb6chuxme7d5bp4zp/lib64:/cluster/apps/gcc-8.2.0/hdf5-1.10.1-qj3ju3qfhvucsk5eevrtb2lehbux5nmv/lib:/cluster/apps/gcc-8.2.0/npm-6.14.9-774crfohwvu6a33ijcow7x5cvonu44oi/lib:/cluster/apps/gcc-8.2.0/r-4.2.2-ydfaklhfrhw5dy6qcfzxlxfviwovcord/rlib/R/lib:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/lib64:/cluster/spack/apps/linux-centos7-x86_64/gcc-4.8.5/gcc-8.2.0-6xqov2fhvbmehix42slain67vprec3fs/lib:/cluster/apps/lsf/10.1/linux2.6-glibc2.3-x86_64/lib::\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "echo $LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8e135fa-05dc-484f-918f-247e6825da36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/project/treutlein/jjans/software/miniforge3/envs/chrombpnet\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "echo $CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57de1bac-af80-46ba-ab8c-cba22b2dc34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57bd72f6-443a-4395-b00f-dc73b9771ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_PrepareBamFiles_merge_species.ipynb\tbam_per_cell_type_per_species.tar\n",
      "01_MACS_PeakCalling.ipynb\t\tbias_model\n",
      "02_PeakManipulation.ipynb\t\tbias_model_own\n",
      "03_MakeDataSplits.ipynb\t\t\tcalled_peaks\n",
      "04_TrainingModel.ipynb\t\t\tencode_data\n",
      "05_BiasModel.ipynb\t\t\tenterocytes_species\n",
      "Untitled.ipynb\t\t\t\tmodelv0\n",
      "Untitled1.ipynb\t\t\t\tuntitled.txt\n",
      "bam_per_cell_type_per_species\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd2042dc-b45b-4b7a-88b1-1b4ccff042a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: chrombpnet [-h]\n",
      "                  {pipeline,train,qc,bias,prep,pred_bw,contribs_bw,modisco_motifs,footprints,snp_score}\n",
      "                  ...\n",
      "\n",
      "==================================================================================================\n",
      "\t\tBias factorized, base-resolution deep learning models of chromatin accessibility reveal \n",
      "\t\tcis-regulatory sequence syntax, transcription factor footprints and regulatory variants\n",
      "\t\t==================================================================================================\n",
      "\n",
      "positional arguments:\n",
      "  {pipeline,train,qc,bias,prep,pred_bw,contribs_bw,modisco_motifs,footprints,snp_score}\n",
      "                        Must be eithier 'pipeline', 'train', 'qc', 'bias', 'prep', 'pred_bw', 'contribs_bw', 'modisco_motifs' ,'footprints', or 'snp_score'.\n",
      "    pipeline            End-to-end pipline with train, quality check and test for bias factorized ChromBPNet model\n",
      "    train               Train bias factorized ChromBPNet model\n",
      "    qc                  Do quality checks and get test metrics for bias factorized ChromBPNet model\n",
      "    bias                Tools to train, quality check and test bias model\n",
      "    prep                Tools to generate preprocessing data for chrombpnet\n",
      "    pred_bw             Get model prediction bigwigs (Metrics calculated if observed bigwig provided)\n",
      "    contribs_bw         Get contribution score bigwigs\n",
      "    modisco_motifs      (Will soon be deprecated: use modisco motifs from tfmodisco lite) Summarize motifs from contribution scores with TFModisco\n",
      "    footprints          Get marginal footprinting for given model and given motifs\n",
      "    snp_score           Score SNPs with model\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "chrombpnet -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f0485b8-0449-4576-9859-6d5bb380e82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: chrombpnet bias pipeline [-h] -g GENOME -c CHROM_SIZES\n",
      "                                (-ibam INPUT_BAM_FILE | -ifrag INPUT_FRAGMENT_FILE | -itag INPUT_TAGALIGN_FILE)\n",
      "                                -o OUTPUT_DIR -d {ATAC,DNASE} -p PEAKS -n\n",
      "                                NONPEAKS -fl CHR_FOLD_PATH\n",
      "                                [-oth OUTLIER_THRESHOLD]\n",
      "                                [--ATAC-ref-path ATAC_REF_PATH]\n",
      "                                [--DNASE-ref-path DNASE_REF_PATH]\n",
      "                                [--num-samples NUM_SAMPLES] [-il INPUTLEN]\n",
      "                                [-ol OUTPUTLEN] [-s SEED] [-e EPOCHS]\n",
      "                                [-es EARLY_STOP] [-l LEARNING_RATE]\n",
      "                                [-track [TRACKABLES ...]]\n",
      "                                [-a ARCHITECTURE_FROM_FILE] [-fp FILE_PREFIX]\n",
      "                                [-hp HTML_PREFIX] [--bsort] [--tmpdir TMPDIR]\n",
      "                                [--no-st] -b BIAS_THRESHOLD_FACTOR\n",
      "                                [-fil FILTERS] [-dil N_DILATION_LAYERS]\n",
      "                                [-j MAX_JITTER] [-bs BATCH_SIZE]\n",
      "\n",
      "required arguments:\n",
      "  -g GENOME, --genome GENOME\n",
      "                        reference genome fasta file\n",
      "  -c CHROM_SIZES, --chrom-sizes CHROM_SIZES\n",
      "                        Chrom sizes file\n",
      "  -ibam INPUT_BAM_FILE, --input-bam-file INPUT_BAM_FILE\n",
      "                        Input BAM file\n",
      "  -ifrag INPUT_FRAGMENT_FILE, --input-fragment-file INPUT_FRAGMENT_FILE\n",
      "                        Input fragment file\n",
      "  -itag INPUT_TAGALIGN_FILE, --input-tagalign-file INPUT_TAGALIGN_FILE\n",
      "                        Input tagAlign file\n",
      "  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n",
      "                        Output dir (path/to/output/dir)\n",
      "  -d {ATAC,DNASE}, --data-type {ATAC,DNASE}\n",
      "                        assay type\n",
      "  -p PEAKS, --peaks PEAKS\n",
      "                        10 column bed file of peaks. Sequences and labels will\n",
      "                        be extracted centered at start (2nd col) + summit\n",
      "                        (10th col).\n",
      "  -n NONPEAKS, --nonpeaks NONPEAKS\n",
      "                        10 column bed file of non-peak regions, centered at\n",
      "                        summit (10th column)\n",
      "  -fl CHR_FOLD_PATH, --chr-fold-path CHR_FOLD_PATH\n",
      "                        Fold information - dictionary with test,valid and\n",
      "                        train keys and values with corresponding chromosomes\n",
      "  -b BIAS_THRESHOLD_FACTOR, --bias-threshold-factor BIAS_THRESHOLD_FACTOR\n",
      "                        A threshold is applied on maximum count of non-peak\n",
      "                        region for training bias model, which is set as this\n",
      "                        threshold x min(count over peak regions). Recommended\n",
      "                        start value 0.5 for ATAC and 0.8 for DNase.\n",
      "\n",
      "optional arguments:\n",
      "  -oth OUTLIER_THRESHOLD, --outlier-threshold OUTLIER_THRESHOLD\n",
      "                        threshold to use to filter outlies\n",
      "  --ATAC-ref-path ATAC_REF_PATH\n",
      "                        Path to ATAC reference motifs (ATAC.ref.motifs.txt\n",
      "                        used by default)\n",
      "  --DNASE-ref-path DNASE_REF_PATH\n",
      "                        Path to DNASE reference motifs (DNASE.ref.motifs.txt\n",
      "                        used by default)\n",
      "  --num-samples NUM_SAMPLES\n",
      "                        Number of reads to sample from BAM/fragment/tagAlign\n",
      "                        file for shift estimation\n",
      "  -il INPUTLEN, --inputlen INPUTLEN\n",
      "                        Sequence input length\n",
      "  -ol OUTPUTLEN, --outputlen OUTPUTLEN\n",
      "                        Prediction output length\n",
      "  -s SEED, --seed SEED  seed to use for model training\n",
      "  -e EPOCHS, --epochs EPOCHS\n",
      "                        Maximum epochs to train\n",
      "  -es EARLY_STOP, --early-stop EARLY_STOP\n",
      "                        Early stop limit, corresponds to 'patience' in\n",
      "                        callback\n",
      "  -l LEARNING_RATE, --learning-rate LEARNING_RATE\n",
      "                        Learning rate for model training\n",
      "  -track [TRACKABLES ...], --trackables [TRACKABLES ...]\n",
      "                        list of things to track per batch, such as logcount_pr\n",
      "                        edictions_loss,loss,profile_predictions_loss,val_logco\n",
      "                        unt_predictions_loss,val_loss,val_profile_predictions_\n",
      "                        loss\n",
      "  -a ARCHITECTURE_FROM_FILE, --architecture-from-file ARCHITECTURE_FROM_FILE\n",
      "                        Model to use for training\n",
      "  -fp FILE_PREFIX, --file-prefix FILE_PREFIX\n",
      "                        File prefix for output to use. All the files will be\n",
      "                        prefixed with this string if provided.\n",
      "  -hp HTML_PREFIX, --html-prefix HTML_PREFIX\n",
      "                        The html prefix to use for the html file output.\n",
      "  --bsort               In prpeprocess, by deafult we sort bam using unix sort\n",
      "                        but sometimes LC collate can cause issues, so this can\n",
      "                        be set to use betools sort which works well but is\n",
      "                        memory intensive..\n",
      "  --tmpdir TMPDIR       temp dir for unix sort\n",
      "  --no-st               Dont do streaming and filtering in preprocessing\n",
      "                        (short chromosome contrigs not in reference fasta are\n",
      "                        not removed)\n",
      "  -fil FILTERS, --filters FILTERS\n",
      "                        Number of filters to use in chrombpnet mode\n",
      "  -dil N_DILATION_LAYERS, --n-dilation-layers N_DILATION_LAYERS\n",
      "                        Number of dilation layers to use in chrombpnet model\n",
      "  -j MAX_JITTER, --max-jitter MAX_JITTER\n",
      "                        Maximum jitter applied on either side of region\n",
      "                        (default 500 for chrombpnet model)\n",
      "  -bs BATCH_SIZE, --batch-size BATCH_SIZE\n",
      "                        batch size to use for model training\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "chrombpnet bias pipeline -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff47079-3200-4d8e-8a86-54ba5416710d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating enzyme shift in input file\n",
      "/cluster/home/jjanssens/.local/lib/python3.10/site-packages/chrombpnet/helpers/preprocessing/auto_shift_detect.py:108: UserWarning: \u001b[31m!!! WARNING: Input reads contain chromosomes not in the reference genome fasta provided. Please ensure you are using the correct reference genome. If you are confident you are using the correct reference genome, you can safely ignore this message.\u001b[0m\n",
      "  warnings.warn(colored(msg, 'red'))\n",
      "Current estimated shift: +0/+0\n",
      "awk -v OFS=\"\\t\" '{if ($6==\"+\"){print $1,$2+4,$3,$4,$5,$6} else if ($6==\"-\") {print $1,$2,$3-4,$4,$5,$6}}' | sort -k1,1 | bedtools genomecov -bg -5 -i stdin -g encode_data/hg38.chrom.sizes | LC_COLLATE=\"C\" sort -k1,1 -k2,2n \n",
      "Making BedGraph (Filter chromosomes not in reference fasta)\n",
      "Making Bigwig\n",
      "non zero bigwig entries in the given chromosome:  2407531\n",
      "evaluating hyperparameters on the following chromosomes ['chr2', 'chr4', 'chr5', 'chr7', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr21', 'chr22', 'chrX', 'chrY', 'chr8', 'chr20']\n",
      "Number of non peaks input:  138188\n",
      "Number of non peaks filtered because the input/output is on the edge:  0\n",
      "Number of non peaks being used:  138188\n",
      "Number of non peaks input:  20090\n",
      "Number of non peaks filtered because the input/output is on the edge:  0\n",
      "Number of non peaks being used:  20090\n",
      "Number of peaks input:  69094\n",
      "Number of peaks filtered because the input/output is on the edge:  0\n",
      "Number of peaks being used:  69094\n",
      "Number of peaks input:  20090\n",
      "Number of peaks filtered because the input/output is on the edge:  0\n",
      "Number of peaks being used:  20090\n",
      "Upper bound counts cut-off for bias model training:  24.0\n",
      "Number of nonpeaks after the upper-bount cut-off:  98069\n",
      "Number of nonpeaks after applying upper-bound cut-off and removing outliers :  89702\n",
      "counts_loss_weight: 1.0\n",
      "{'counts_loss_weight': '1.0', 'filters': '128', 'n_dil_layers': '4', 'inputlen': '2114', 'outputlen': '1000', 'max_jitter': '0', 'chr_fold_path': 'encode_data/splits/fold_0.json', 'negative_sampling_ratio': '1.0'}\n",
      "params:\n",
      "filters:128\n",
      "n_dil_layers:4\n",
      "conv1_kernel_size:21\n",
      "profile_kernel_size:75\n",
      "counts_loss_weight:1.0\n",
      "got the model\n",
      "loading nonpeaks...\n",
      "got split:train for bed regions:(81380, 10)\n",
      "loading nonpeaks...\n",
      "got split:valid for bed regions:(8322, 10)\n",
      "Epoch 1/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 46.4087 - logits_profile_predictions_loss: 46.0895 - logcount_predictions_loss: 0.31920  \n",
      "Epoch 1: val_loss improved from inf to 47.53471, saving model to bias_model_own/models/bias.h5\n",
      "1272/1272 [==============================] - 73s 38ms/step - loss: 46.4100 - logits_profile_predictions_loss: 46.0907 - logcount_predictions_loss: 0.3193 - val_loss: 47.5347 - val_logits_profile_predictions_loss: 47.2354 - val_logcount_predictions_loss: 0.2993\n",
      "Epoch 2/50\n",
      "1272/1272 [==============================] - ETA: 0s - loss: 46.0205 - logits_profile_predictions_loss: 45.7181 - logcount_predictions_loss: 0.3024\n",
      "Epoch 2: val_loss improved from 47.53471 to 47.42199, saving model to bias_model_own/models/bias.h5\n",
      "1272/1272 [==============================] - 48s 38ms/step - loss: 46.0205 - logits_profile_predictions_loss: 45.7181 - logcount_predictions_loss: 0.3024 - val_loss: 47.4220 - val_logits_profile_predictions_loss: 47.1367 - val_logcount_predictions_loss: 0.2853\n",
      "Epoch 3/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.9392 - logits_profile_predictions_loss: 45.6404 - logcount_predictions_loss: 0.29897\n",
      "Epoch 3: val_loss improved from 47.42199 to 47.36781, saving model to bias_model_own/models/bias.h5\n",
      "1272/1272 [==============================] - 49s 38ms/step - loss: 45.9431 - logits_profile_predictions_loss: 45.6443 - logcount_predictions_loss: 0.2988 - val_loss: 47.3678 - val_logits_profile_predictions_loss: 47.1045 - val_logcount_predictions_loss: 0.2634\n",
      "Epoch 4/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.8984 - logits_profile_predictions_loss: 45.6018 - logcount_predictions_loss: 0.2966\n",
      "Epoch 4: val_loss improved from 47.36781 to 47.33844, saving model to bias_model_own/models/bias.h5\n",
      "1272/1272 [==============================] - 48s 38ms/step - loss: 45.8985 - logits_profile_predictions_loss: 45.6020 - logcount_predictions_loss: 0.2965 - val_loss: 47.3384 - val_logits_profile_predictions_loss: 47.0824 - val_logcount_predictions_loss: 0.2561\n",
      "Epoch 5/50\n",
      "1272/1272 [==============================] - ETA: 0s - loss: 45.8663 - logits_profile_predictions_loss: 45.5713 - logcount_predictions_loss: 0.29497\n",
      "Epoch 5: val_loss improved from 47.33844 to 47.29851, saving model to bias_model_own/models/bias.h5\n",
      "1272/1272 [==============================] - 49s 38ms/step - loss: 45.8663 - logits_profile_predictions_loss: 45.5713 - logcount_predictions_loss: 0.2949 - val_loss: 47.2985 - val_logits_profile_predictions_loss: 47.0436 - val_logcount_predictions_loss: 0.2549\n",
      "Epoch 6/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.8412 - logits_profile_predictions_loss: 45.5472 - logcount_predictions_loss: 0.29407\n",
      "Epoch 6: val_loss did not improve from 47.29851\n",
      "1272/1272 [==============================] - 49s 38ms/step - loss: 45.8410 - logits_profile_predictions_loss: 45.5470 - logcount_predictions_loss: 0.2940 - val_loss: 47.3142 - val_logits_profile_predictions_loss: 47.0598 - val_logcount_predictions_loss: 0.2544\n",
      "Epoch 7/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.8188 - logits_profile_predictions_loss: 45.5262 - logcount_predictions_loss: 0.2926\n",
      "Epoch 7: val_loss did not improve from 47.29851\n",
      "1272/1272 [==============================] - 48s 38ms/step - loss: 45.8195 - logits_profile_predictions_loss: 45.5269 - logcount_predictions_loss: 0.2926 - val_loss: 47.3302 - val_logits_profile_predictions_loss: 47.0753 - val_logcount_predictions_loss: 0.2549\n",
      "Epoch 8/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.8000 - logits_profile_predictions_loss: 45.5080 - logcount_predictions_loss: 0.29202\n",
      "Epoch 8: val_loss improved from 47.29851 to 47.29361, saving model to bias_model_own/models/bias.h5\n",
      "1272/1272 [==============================] - 49s 38ms/step - loss: 45.7988 - logits_profile_predictions_loss: 45.5067 - logcount_predictions_loss: 0.2921 - val_loss: 47.2936 - val_logits_profile_predictions_loss: 47.0438 - val_logcount_predictions_loss: 0.2498\n",
      "Epoch 9/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.7816 - logits_profile_predictions_loss: 45.4902 - logcount_predictions_loss: 0.29147\n",
      "Epoch 9: val_loss did not improve from 47.29361\n",
      "1272/1272 [==============================] - 48s 38ms/step - loss: 45.7823 - logits_profile_predictions_loss: 45.4909 - logcount_predictions_loss: 0.2913 - val_loss: 47.3091 - val_logits_profile_predictions_loss: 47.0566 - val_logcount_predictions_loss: 0.2525\n",
      "Epoch 10/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.7639 - logits_profile_predictions_loss: 45.4732 - logcount_predictions_loss: 0.29086\n",
      "Epoch 10: val_loss did not improve from 47.29361\n",
      "1272/1272 [==============================] - 49s 38ms/step - loss: 45.7663 - logits_profile_predictions_loss: 45.4756 - logcount_predictions_loss: 0.2907 - val_loss: 47.3017 - val_logits_profile_predictions_loss: 47.0500 - val_logcount_predictions_loss: 0.2517\n",
      "Epoch 11/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.7493 - logits_profile_predictions_loss: 45.4593 - logcount_predictions_loss: 0.29006\n",
      "Epoch 11: val_loss did not improve from 47.29361\n",
      "1272/1272 [==============================] - 49s 38ms/step - loss: 45.7476 - logits_profile_predictions_loss: 45.4576 - logcount_predictions_loss: 0.2900 - val_loss: 47.3359 - val_logits_profile_predictions_loss: 47.0757 - val_logcount_predictions_loss: 0.2602\n",
      "Epoch 12/50\n",
      "1272/1272 [==============================] - ETA: 0s - loss: 45.7347 - logits_profile_predictions_loss: 45.4458 - logcount_predictions_loss: 0.28900\n",
      "Epoch 12: val_loss did not improve from 47.29361\n",
      "1272/1272 [==============================] - 48s 38ms/step - loss: 45.7347 - logits_profile_predictions_loss: 45.4458 - logcount_predictions_loss: 0.2890 - val_loss: 47.3071 - val_logits_profile_predictions_loss: 47.0571 - val_logcount_predictions_loss: 0.2499\n",
      "Epoch 13/50\n",
      "1271/1272 [============================>.] - ETA: 0s - loss: 45.7111 - logits_profile_predictions_loss: 45.4233 - logcount_predictions_loss: 0.28786\n",
      "Epoch 13: val_loss did not improve from 47.29361\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "1272/1272 [==============================] - 49s 38ms/step - loss: 45.7137 - logits_profile_predictions_loss: 45.4259 - logcount_predictions_loss: 0.2878 - val_loss: 47.3122 - val_logits_profile_predictions_loss: 47.0613 - val_logcount_predictions_loss: 0.2509\n",
      "Epoch 13: early stopping\n",
      "save model\n",
      "got the model\n",
      "loading peaks...\n",
      "got split:test for bed regions:(20090, 10)\n",
      "loading nonpeaks...\n",
      "got split:test for bed regions:(20090, 10)\n",
      "0/628\n",
      "2024-04-24 17:10:40.874545: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "100/628\n",
      "200/628\n",
      "300/628\n",
      "2024-04-24 17:10:45.638490: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2024-04-24 17:10:46.776861: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "400/628\n",
      "2024-04-24 17:10:49.397990: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "500/628\n",
      "2024-04-24 17:10:49.573685: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2024-04-24 17:10:50.432916: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "2024-04-24 17:10:50.973408: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n",
      "600/628\n",
      "/cluster/apps/nss/jupyterhub/3.5.1/lib64/python3.10/site-packages/scipy/spatial/distance.py:1249: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n",
      "/cluster/home/jjanssens/.local/lib/python3.10/site-packages/chrombpnet/training/utils/metrics_utils.py:196: RuntimeWarning: invalid value encountered in divide\n",
      "  profile_prob = profile / np.sum(profile)\n",
      "/cluster/apps/nss/jupyterhub/3.5.1/lib64/python3.10/site-packages/scipy/spatial/distance.py:1250: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "got the model\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 2114, 4)]    0           []                               \n",
      "                                                                                                  \n",
      " bpnet_1st_conv (Conv1D)        (None, 2094, 128)    10880       ['sequence[0][0]']               \n",
      "                                                                                                  \n",
      " bpnet_1conv (Conv1D)           (None, 2090, 128)    49280       ['bpnet_1st_conv[0][0]']         \n",
      "                                                                                                  \n",
      " bpnet_1crop (Cropping1D)       (None, 2090, 128)    0           ['bpnet_1st_conv[0][0]']         \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 2090, 128)    0           ['bpnet_1conv[0][0]',            \n",
      "                                                                  'bpnet_1crop[0][0]']            \n",
      "                                                                                                  \n",
      " bpnet_2conv (Conv1D)           (None, 2082, 128)    49280       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " bpnet_2crop (Cropping1D)       (None, 2082, 128)    0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 2082, 128)    0           ['bpnet_2conv[0][0]',            \n",
      "                                                                  'bpnet_2crop[0][0]']            \n",
      "                                                                                                  \n",
      " bpnet_3conv (Conv1D)           (None, 2066, 128)    49280       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " bpnet_3crop (Cropping1D)       (None, 2066, 128)    0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 2066, 128)    0           ['bpnet_3conv[0][0]',            \n",
      "                                                                  'bpnet_3crop[0][0]']            \n",
      "                                                                                                  \n",
      " bpnet_4conv (Conv1D)           (None, 2034, 128)    49280       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " bpnet_4crop (Cropping1D)       (None, 2034, 128)    0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 2034, 128)    0           ['bpnet_4conv[0][0]',            \n",
      "                                                                  'bpnet_4crop[0][0]']            \n",
      "                                                                                                  \n",
      " prof_out_precrop (Conv1D)      (None, 1960, 1)      9601        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " logits_profile_predictions_pre  (None, 1000, 1)     0           ['prof_out_precrop[0][0]']       \n",
      " flatten (Cropping1D)                                                                             \n",
      "                                                                                                  \n",
      " gap (GlobalAveragePooling1D)   (None, 128)          0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " logits_profile_predictions (Fl  (None, 1000)        0           ['logits_profile_predictions_pref\n",
      " atten)                                                          latten[0][0]']                   \n",
      "                                                                                                  \n",
      " logcount_predictions (Dense)   (None, 1)            129         ['gap[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 217,730\n",
      "Trainable params: 217,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "inferred model inputlen:  2114\n",
      "Seqs dimension : (30000, 2114, 4)\n",
      "WARNING:tensorflow:From /cluster/home/jjanssens/.local/lib/python3.10/site-packages/shap/explainers/deep/deep_tf.py:140: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n",
      "Warning:  AddV2 used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  AddV2 used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  AddV2 used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  AddV2 used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  SpaceToBatchND used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  BatchToSpaceND used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  SpaceToBatchND used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  BatchToSpaceND used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  SpaceToBatchND used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  BatchToSpaceND used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  SpaceToBatchND used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Warning:  BatchToSpaceND used in model but handling of op is not specified by shap; will use original  gradients\n",
      "Generating 'counts' shap scores\n",
      "Done 0 examples of 30000\n",
      "Done 100 examples of 30000\n",
      "Done 200 examples of 30000\n",
      "Done 300 examples of 30000\n",
      "Done 400 examples of 30000\n",
      "Done 500 examples of 30000\n",
      "Done 600 examples of 30000\n",
      "Done 700 examples of 30000\n",
      "Done 800 examples of 30000\n",
      "Done 900 examples of 30000\n",
      "Done 1000 examples of 30000\n",
      "Done 1100 examples of 30000\n",
      "Done 1200 examples of 30000\n",
      "Done 1300 examples of 30000\n",
      "Done 1400 examples of 30000\n",
      "Done 1500 examples of 30000\n",
      "Done 1600 examples of 30000\n",
      "Done 1700 examples of 30000\n",
      "Done 1800 examples of 30000\n",
      "Done 1900 examples of 30000\n",
      "Done 2000 examples of 30000\n",
      "Done 2100 examples of 30000\n",
      "Done 2200 examples of 30000\n",
      "Done 2300 examples of 30000\n",
      "Done 2400 examples of 30000\n",
      "Done 2500 examples of 30000\n",
      "Done 2600 examples of 30000\n",
      "Done 2700 examples of 30000\n",
      "Done 2800 examples of 30000\n",
      "Done 2900 examples of 30000\n",
      "Done 3000 examples of 30000\n",
      "Done 3100 examples of 30000\n",
      "Done 3200 examples of 30000\n",
      "Done 3300 examples of 30000\n",
      "Done 3400 examples of 30000\n",
      "Done 3500 examples of 30000\n",
      "Done 3600 examples of 30000\n",
      "Done 3700 examples of 30000\n",
      "Done 3800 examples of 30000\n",
      "Done 3900 examples of 30000\n",
      "Done 4000 examples of 30000\n",
      "Done 4100 examples of 30000\n",
      "Done 4200 examples of 30000\n",
      "Done 4300 examples of 30000\n",
      "Done 4400 examples of 30000\n",
      "Done 4500 examples of 30000\n",
      "Done 4600 examples of 30000\n",
      "Done 4700 examples of 30000\n",
      "Done 4800 examples of 30000\n",
      "Done 4900 examples of 30000\n",
      "Done 5000 examples of 30000\n",
      "Done 5100 examples of 30000\n",
      "Done 5200 examples of 30000\n",
      "Done 5300 examples of 30000\n",
      "Done 5400 examples of 30000\n",
      "Done 5500 examples of 30000\n",
      "Done 5600 examples of 30000\n",
      "Done 5700 examples of 30000\n",
      "Done 5800 examples of 30000\n",
      "Done 5900 examples of 30000\n",
      "Done 6000 examples of 30000\n",
      "Done 6100 examples of 30000\n",
      "Done 6200 examples of 30000\n",
      "Done 6300 examples of 30000\n",
      "Done 6400 examples of 30000\n",
      "Done 6500 examples of 30000\n",
      "Done 6600 examples of 30000\n",
      "Done 6700 examples of 30000\n",
      "Done 6800 examples of 30000\n",
      "Done 6900 examples of 30000\n",
      "Done 7000 examples of 30000\n",
      "Done 7100 examples of 30000\n",
      "Done 7200 examples of 30000\n",
      "Done 7300 examples of 30000\n",
      "Done 7400 examples of 30000\n",
      "Done 7500 examples of 30000\n",
      "Done 7600 examples of 30000\n",
      "Done 7700 examples of 30000\n",
      "Done 7800 examples of 30000\n",
      "Done 7900 examples of 30000\n",
      "Done 8000 examples of 30000\n",
      "Done 8100 examples of 30000\n",
      "Done 8200 examples of 30000\n",
      "Done 8300 examples of 30000\n",
      "Done 8400 examples of 30000\n",
      "Done 8500 examples of 30000\n",
      "Done 8600 examples of 30000\n",
      "Done 8700 examples of 30000\n",
      "Done 8800 examples of 30000\n",
      "Done 8900 examples of 30000\n",
      "Done 9000 examples of 30000\n",
      "Done 9100 examples of 30000\n",
      "Done 9200 examples of 30000\n",
      "Done 9300 examples of 30000\n",
      "Done 9400 examples of 30000\n",
      "Done 9500 examples of 30000\n",
      "Done 9600 examples of 30000\n",
      "Done 9700 examples of 30000\n",
      "Done 9800 examples of 30000\n",
      "Done 9900 examples of 30000\n",
      "Done 10000 examples of 30000\n",
      "Done 10100 examples of 30000\n",
      "Done 10200 examples of 30000\n",
      "Done 10300 examples of 30000\n",
      "Done 10400 examples of 30000\n",
      "Done 10500 examples of 30000\n",
      "Done 10600 examples of 30000\n",
      "Done 10700 examples of 30000\n",
      "Done 10800 examples of 30000\n",
      "Done 10900 examples of 30000\n",
      "Done 11000 examples of 30000\n",
      "Done 11100 examples of 30000\n",
      "Done 11200 examples of 30000\n",
      "Done 11300 examples of 30000\n",
      "Done 11400 examples of 30000\n",
      "Done 11500 examples of 30000\n",
      "Done 11600 examples of 30000\n",
      "Done 11700 examples of 30000\n",
      "Done 11800 examples of 30000\n",
      "Done 11900 examples of 30000\n",
      "Done 12000 examples of 30000\n",
      "Done 12100 examples of 30000\n",
      "Done 12200 examples of 30000\n",
      "Done 12300 examples of 30000\n",
      "Done 12400 examples of 30000\n",
      "Done 12500 examples of 30000\n",
      "Done 12600 examples of 30000\n",
      "Done 12700 examples of 30000\n",
      "Done 12800 examples of 30000\n",
      "Done 12900 examples of 30000\n",
      "Done 13000 examples of 30000\n",
      "Done 13100 examples of 30000\n",
      "Done 13200 examples of 30000\n",
      "Done 13300 examples of 30000\n",
      "Done 13400 examples of 30000\n",
      "Done 13500 examples of 30000\n",
      "Done 13600 examples of 30000\n",
      "Done 13700 examples of 30000\n",
      "Done 13800 examples of 30000\n",
      "Done 13900 examples of 30000\n",
      "Done 14000 examples of 30000\n",
      "Done 14100 examples of 30000\n",
      "Done 14200 examples of 30000\n",
      "Done 14300 examples of 30000\n",
      "Done 14400 examples of 30000\n",
      "Done 14500 examples of 30000\n",
      "Done 14600 examples of 30000\n",
      "Done 14700 examples of 30000\n",
      "Done 14800 examples of 30000\n",
      "Done 14900 examples of 30000\n",
      "Done 15000 examples of 30000\n",
      "Done 15100 examples of 30000\n",
      "Done 15200 examples of 30000\n",
      "Done 15300 examples of 30000\n",
      "Done 15400 examples of 30000\n",
      "Done 15500 examples of 30000\n",
      "Done 15600 examples of 30000\n",
      "Done 15700 examples of 30000\n",
      "Done 15800 examples of 30000\n",
      "Done 15900 examples of 30000\n",
      "Done 16000 examples of 30000\n",
      "Done 16100 examples of 30000\n",
      "Done 16200 examples of 30000\n",
      "Done 16300 examples of 30000\n",
      "Done 16400 examples of 30000\n",
      "Done 16500 examples of 30000\n",
      "Done 16600 examples of 30000\n",
      "Done 16700 examples of 30000\n",
      "Done 16800 examples of 30000\n",
      "Done 16900 examples of 30000\n",
      "Done 17000 examples of 30000\n",
      "Done 17100 examples of 30000\n",
      "Done 17200 examples of 30000\n",
      "Done 17300 examples of 30000\n",
      "Done 17400 examples of 30000\n",
      "Done 17500 examples of 30000\n",
      "Done 17600 examples of 30000\n",
      "Done 17700 examples of 30000\n",
      "Done 17800 examples of 30000\n",
      "Done 17900 examples of 30000\n",
      "Done 18000 examples of 30000\n",
      "Done 18100 examples of 30000\n",
      "Done 18200 examples of 30000\n",
      "Done 18300 examples of 30000\n",
      "Done 18400 examples of 30000\n",
      "Done 18500 examples of 30000\n",
      "Done 18600 examples of 30000\n",
      "Done 18700 examples of 30000\n",
      "Done 18800 examples of 30000\n",
      "Done 18900 examples of 30000\n",
      "Done 19000 examples of 30000\n",
      "Done 19100 examples of 30000\n",
      "Done 19200 examples of 30000\n",
      "Done 19300 examples of 30000\n",
      "Done 19400 examples of 30000\n",
      "Done 19500 examples of 30000\n",
      "Done 19600 examples of 30000\n",
      "Done 19700 examples of 30000\n",
      "Done 19800 examples of 30000\n",
      "Done 19900 examples of 30000\n",
      "Done 20000 examples of 30000\n",
      "Done 20100 examples of 30000\n",
      "Done 20200 examples of 30000\n",
      "Done 20300 examples of 30000\n",
      "Done 20400 examples of 30000\n",
      "Done 20500 examples of 30000\n",
      "Done 20600 examples of 30000\n",
      "Done 20700 examples of 30000\n",
      "Done 20800 examples of 30000\n",
      "Done 20900 examples of 30000\n",
      "Done 21000 examples of 30000\n",
      "Done 21100 examples of 30000\n",
      "Done 21200 examples of 30000\n",
      "Done 21300 examples of 30000\n",
      "Done 21400 examples of 30000\n",
      "Done 21500 examples of 30000\n",
      "Done 21600 examples of 30000\n",
      "Done 21700 examples of 30000\n",
      "Done 21800 examples of 30000\n",
      "Done 21900 examples of 30000\n",
      "Done 22000 examples of 30000\n",
      "Done 22100 examples of 30000\n",
      "Done 22200 examples of 30000\n",
      "Done 22300 examples of 30000\n",
      "Done 22400 examples of 30000\n",
      "Done 22500 examples of 30000\n",
      "Done 22600 examples of 30000\n",
      "Done 22700 examples of 30000\n",
      "Done 22800 examples of 30000\n",
      "Done 22900 examples of 30000\n",
      "Done 23000 examples of 30000\n",
      "Done 23100 examples of 30000\n",
      "Done 23200 examples of 30000\n",
      "Done 23300 examples of 30000\n",
      "Done 23400 examples of 30000\n",
      "Done 23500 examples of 30000\n",
      "Done 23600 examples of 30000\n",
      "Done 23700 examples of 30000\n",
      "Done 23800 examples of 30000\n",
      "Done 23900 examples of 30000\n",
      "Done 24000 examples of 30000\n",
      "Done 24100 examples of 30000\n",
      "Done 24200 examples of 30000\n",
      "Done 24300 examples of 30000\n",
      "Done 24400 examples of 30000\n",
      "Done 24500 examples of 30000\n",
      "Done 24600 examples of 30000\n",
      "Done 24700 examples of 30000\n",
      "Done 24800 examples of 30000\n",
      "Done 24900 examples of 30000\n",
      "Done 25000 examples of 30000\n",
      "Done 25100 examples of 30000\n",
      "Done 25200 examples of 30000\n",
      "Done 25300 examples of 30000\n",
      "Done 25400 examples of 30000\n",
      "Done 25500 examples of 30000\n",
      "Done 25600 examples of 30000\n",
      "Done 25700 examples of 30000\n",
      "Done 25800 examples of 30000\n",
      "Done 25900 examples of 30000\n",
      "Done 26000 examples of 30000\n",
      "Done 26100 examples of 30000\n",
      "Done 26200 examples of 30000\n",
      "Done 26300 examples of 30000\n",
      "Done 26400 examples of 30000\n",
      "Done 26500 examples of 30000\n",
      "Done 26600 examples of 30000\n",
      "Done 26700 examples of 30000\n",
      "Done 26800 examples of 30000\n",
      "Done 26900 examples of 30000\n",
      "Done 27000 examples of 30000\n",
      "Done 27100 examples of 30000\n",
      "Done 27200 examples of 30000\n",
      "Done 27300 examples of 30000\n",
      "Done 27400 examples of 30000\n",
      "Done 27500 examples of 30000\n",
      "Done 27600 examples of 30000\n",
      "Done 27700 examples of 30000\n",
      "Done 27800 examples of 30000\n",
      "Done 27900 examples of 30000\n",
      "Done 28000 examples of 30000\n",
      "Done 28100 examples of 30000\n",
      "Done 28200 examples of 30000\n",
      "Done 28300 examples of 30000\n",
      "Done 28400 examples of 30000\n",
      "Done 28500 examples of 30000\n",
      "Done 28600 examples of 30000\n",
      "Done 28700 examples of 30000\n",
      "Done 28800 examples of 30000\n",
      "Done 28900 examples of 30000\n",
      "Done 29000 examples of 30000\n",
      "Done 29100 examples of 30000\n",
      "Done 29200 examples of 30000\n",
      "Done 29300 examples of 30000\n",
      "Done 29400 examples of 30000\n",
      "Done 29500 examples of 30000\n",
      "Done 29600 examples of 30000\n",
      "Done 29700 examples of 30000\n",
      "Done 29800 examples of 30000\n",
      "Done 29900 examples of 30000\n",
      "Saving 'counts' scores\n",
      "/cluster/home/jjanssens/.local/lib/python3.10/site-packages/deepdish/io/hdf5io.py:125: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  elif x.dtype == np.object:\n",
      "Traceback (most recent call last):\n",
      "  File \"/cluster/home/jjanssens/.local/bin/chrombpnet\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/cluster/home/jjanssens/.local/lib/python3.10/site-packages/chrombpnet/CHROMBPNET.py\", line 38, in main\n",
      "    pipelines.train_bias_pipeline(args)\n",
      "  File \"/cluster/home/jjanssens/.local/lib/python3.10/site-packages/chrombpnet/pipelines.py\", line 354, in train_bias_pipeline\n",
      "    interpret.main(args_copy)\n",
      "  File \"/cluster/home/jjanssens/.local/lib/python3.10/site-packages/chrombpnet/evaluation/interpret/interpret.py\", line 132, in main\n",
      "    interpret(model, seqs, args.output_prefix, args.profile_or_counts)\n",
      "  File \"/cluster/home/jjanssens/.local/lib/python3.10/site-packages/chrombpnet/evaluation/interpret/interpret.py\", line 75, in interpret\n",
      "    dd.io.save(\"{}.counts_scores.h5\".format(output_prefix),\n",
      "  File \"/cluster/home/jjanssens/.local/lib/python3.10/site-packages/deepdish/io/hdf5io.py\", line 584, in save\n",
      "    _save_level(h5file, group, value, name=key,\n",
      "  File \"/cluster/home/jjanssens/.local/lib/python3.10/site-packages/deepdish/io/hdf5io.py\", line 212, in _save_level\n",
      "    _save_level(handler, new_group, v, name=k, filters=filters,\n",
      "  File \"/cluster/home/jjanssens/.local/lib/python3.10/site-packages/deepdish/io/hdf5io.py\", line 250, in _save_level\n",
      "    _save_ndarray(handler, group, name, level, filters=filters)\n",
      "  File \"/cluster/home/jjanssens/.local/lib/python3.10/site-packages/deepdish/io/hdf5io.py\", line 125, in _save_ndarray\n",
      "    elif x.dtype == np.object:\n",
      "  File \"/cluster/apps/nss/jupyterhub/3.5.1/lib64/python3.10/site-packages/numpy/__init__.py\", line 313, in __getattr__\n",
      "    raise AttributeError(__former_attrs__[attr])\n",
      "AttributeError: module 'numpy' has no attribute 'object'.\n",
      "`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n",
      "    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'object_'?\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "chrombpnet bias pipeline \\\n",
    "        -ibam enterocytes_species/enterocyte_human_sorted_filtered.bam \\\n",
    "        -d \"ATAC\" \\\n",
    "        -g encode_data/hg38.fa \\\n",
    "        -c encode_data/hg38.chrom.sizes \\\n",
    "        -p called_peaks/peaks_no_blacklist_chr.bed \\\n",
    "        -n modelv0/output_negatives.bed \\\n",
    "        -fl encode_data/splits/fold_0.json \\\n",
    "        -b 0.5 \\\n",
    "        -o bias_model_own/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd001f46-e52d-47ff-9493-8a8bbd10ecab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2111553a-b94a-43fe-af8d-0a69152674ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/project/treutlein/jjans/software/miniforge3/envs/chrombpnet/bin/python\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9abb4fc-206d-4628-9300-139ae94b7108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25.0\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "python -c \"import numpy; print(numpy.__version__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cf68a-f96c-4c86-b9eb-9760a52bc3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0497bb08-3d40-42c2-b9e1-53e5f351324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25.0\n",
      "(chrombpnet) \n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "python -c \"import numpy; print(numpy.__version__)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b5701-f668-42f7-9773-d09c2086c79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2a215-da1a-47dc-b600-bd2f76763452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
